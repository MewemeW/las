{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T02:53:21.833681Z",
     "start_time": "2021-04-14T02:53:21.827682Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./x.pkl', 'rb') as f:\n",
    "    x = pickle.load(f)\n",
    "with open('./y.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T02:06:44.140309Z",
     "start_time": "2021-04-14T02:06:43.902282Z"
    }
   },
   "outputs": [],
   "source": [
    "X = tf.expand_dims(x, axis=-1).numpy()[:-2]\n",
    "Y = y[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T02:06:44.156253Z",
     "start_time": "2021-04-14T02:06:44.142253Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 584, 129, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T02:06:44.654252Z",
     "start_time": "2021-04-14T02:06:44.158253Z"
    }
   },
   "outputs": [],
   "source": [
    "X = (X - tf.math.reduce_mean(X))/tf.math.reduce_std(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T02:06:44.669253Z",
     "start_time": "2021-04-14T02:06:44.656252Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 435)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T02:06:45.584489Z",
     "start_time": "2021-04-14T02:06:45.560488Z"
    }
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "audio_ds = tf.data.Dataset.from_tensor_slices((X,Y))\n",
    "audio_ds= audio_ds.cache().prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T02:53:07.395299Z",
     "start_time": "2021-04-14T02:53:07.379299Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer): # 为什么这里调用的是layers.Layer? \n",
    "    def __init__(self, units, batch_size, rate=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_size\n",
    "        self.units = units\n",
    "        self.conv1 = layers.Conv2D(32, 3, activation='relu')\n",
    "        self.conv2 = layers.Conv2D(64, 3, activation='relu')\n",
    "        self.pool1 = layers.MaxPooling2D()\n",
    "        self.drop = layers.Dropout(rate)\n",
    "        self.norm = layers.BatchNormalization()\n",
    "        self.pool2 = layers.GlobalMaxPool2D()\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                               return_sequences=True,\n",
    "                               return_state=True,\n",
    "                               dropout = 0.2,\n",
    "                               recurrent_initializer='glorot_uniform')\n",
    "        layers.GRU(units,\n",
    "                   dropout = rate,\n",
    "                   recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "    \n",
    "    def call(self, eninput, hidden):\n",
    "        x = self.conv1(eninput)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.norm(x)\n",
    "#         print('x:',x.shape)\n",
    "        x = self.pool2(x)\n",
    "        x = tf.expand_dims(x,axis=1)\n",
    "#         print('maxpool:',x.shape)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "#         print('EncoderShape:',output.shape)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz,self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T02:53:07.648908Z",
     "start_time": "2021-04-14T02:53:07.635877Z"
    }
   },
   "outputs": [],
   "source": [
    "class Attention(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = layers.Dense(units)\n",
    "        self.W2 = layers.Dense(units)\n",
    "        self.V = layers.Dense(1)\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        # [batch_size, 1, hidden] 为什么\n",
    "        query_time = tf.expand_dims(query, 1)\n",
    "        # [batch_size, units, 1] 为什么只输出一个呢\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_time) + self.W2(values)))\n",
    "        # 在维度为[1]上做softmax\n",
    "        attn_weights = tf.nn.softmax(score , axis=1)\n",
    "        # 对values进行加权，并在维度为[1]上计算其sum\n",
    "        context_vec = attn_weights*values\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "#         print('context_vec_shape:',context_vec.shape)\n",
    "        return context_vec, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T02:53:07.900187Z",
     "start_time": "2021-04-14T02:53:07.882189Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, units, vocab_size, output_dim, rate=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = layers.Embedding(1,output_dim)\n",
    "        self.gru = layers.GRU(units,return_sequences=True, \n",
    "                           return_state=True,dropout = rate,\n",
    "                           recurrent_initializer='glorot_uniform')\n",
    "        self.attn = Attention(units)\n",
    "        self.dense = layers.Dense(vocab_size)\n",
    "    \n",
    "    # TODO: using beam_search和teacher forcing\n",
    "    def call(self, deinput, hidden, enoutput):\n",
    "        # 计算decoder的hidden和encoder的ouput的context_vec\n",
    "        # [batch_size, hidden_size]\n",
    "        context, attn = self.attn(hidden, enoutput)\n",
    "        x = self.embedding(deinput)\n",
    "#         print(\"embeddingShape\",x.shape)\n",
    "        # decoder的输入加上context_vec\n",
    "        # [batch_size,1,hidden_size] + [batch_size,max_len,features]\n",
    "        x = tf.concat([tf.expand_dims(context,axis=1),x], axis=-1)\n",
    "        # output 输出[batch_size,1,hidden_size]\n",
    "        output, state = self.gru(x)\n",
    "        # reshape为 [batch_size, hidden_size]\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        res = self.dense(output)\n",
    "#         print(\"resultShape\",res.shape)\n",
    "\n",
    "        return res "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T02:53:08.450054Z",
     "start_time": "2021-04-14T02:53:08.441056Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_ctc_loss(logits, labels, logit_length, label_length):\n",
    "    return tf.nn.ctc_loss(labels=labels,\n",
    "                           logits=logits,\n",
    "                           logit_length=logit_length,\n",
    "                           label_length=label_length,\n",
    "                           logits_time_major=False,\n",
    "                           unique=None,\n",
    "                           blank_index=-1,\n",
    "                           name=None\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T02:53:10.319983Z",
     "start_time": "2021-04-14T02:53:10.290892Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "# train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型参数初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T02:53:11.016542Z",
     "start_time": "2021-04-14T02:53:10.960541Z"
    }
   },
   "outputs": [],
   "source": [
    "units = 29\n",
    "vocab_size = 29\n",
    "batch_size = 32\n",
    "embedding_dim = 29\n",
    "encoder = Encoder(units, batch_size)\n",
    "attn = Attention(10)\n",
    "decoder = Decoder(units, vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T02:06:49.300088Z",
     "start_time": "2021-04-14T02:06:49.294087Z"
    }
   },
   "outputs": [],
   "source": [
    "audio_ds = audio_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存模型权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T02:53:27.671282Z",
     "start_time": "2021-04-14T02:53:27.659257Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-14T02:53:31.061871Z",
     "start_time": "2021-04-14T02:53:31.051872Z"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y, enc_hidden):\n",
    "    # 使用GradientTape实现梯度下降\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(x, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        # 初始化采用0-29随机整数\n",
    "        dec_input = tf.random.uniform((batch_size,1),maxval=30,dtype=tf.int32)\n",
    "        for t in range(1, y.shape[1]):\n",
    "            logits = tf.expand_dims(decoder(dec_input, dec_hidden, enc_output),1)\n",
    "            label = tf.expand_dims(y[:,t],1)\n",
    "            logit_length = [logits.shape[1]]*logits.shape[0]\n",
    "            label_length = [label.shape[1]]*label.shape[0]\n",
    "            loss = compute_ctc_loss(logits, label, logit_length, label_length)\n",
    "            dec_input = tf.expand_dims(y[:,t],1)\n",
    "    # 计算梯度变量\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    # 梯度下降\n",
    "    grads = tape.gradient(loss, variables)\n",
    "    # 优化\n",
    "    optimizer.apply_gradients(zip(grads, variables))\n",
    "    # loss\n",
    "    train_loss(loss)\n",
    "#     train_accuracy(label, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-14T02:53:31.386Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (32, 290, 62, 64)\n",
      "x: (32, 290, 62, 64)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss.reset_states()\n",
    "#     train_accuracy.reset_states()\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    for x, y in audio_ds:\n",
    "        train_step(x,y,enc_hidden)\n",
    "    # 保存权重    \n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    print(f\"Epoch{epoch+1},\"\n",
    "          f\"Loss:{train_loss.result()}\"\n",
    "#           f\"Accuracy:{train_accuracy.result()*100}\"\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载模型权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T10:37:15.332534Z",
     "start_time": "2021-04-13T10:37:15.325502Z"
    }
   },
   "outputs": [],
   "source": [
    "transcript = 'BUT IN LESS THAN FIVE MINUTES THE STAIRCASE GROANED BENEATH AN EXTRAORDINARY WEIGHT'.lower()\n",
    "sample_call = '84-121123-0001.flac'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T10:37:20.432880Z",
     "start_time": "2021-04-13T10:37:18.253974Z"
    }
   },
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T10:37:20.448874Z",
     "start_time": "2021-04-13T10:37:20.433781Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_spectrogram(signals):\n",
    "    '''\n",
    "    function to create spectrogram from signals loaded from an audio file\n",
    "    :param signals:\n",
    "    :return:\n",
    "    '''\n",
    "    stfts = tf.signal.stft(signals, frame_length=200, frame_step=80, fft_length=256)\n",
    "    spectrograms = tf.math.pow(tf.abs(stfts), 0.5)\n",
    "    return spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T10:37:20.464874Z",
     "start_time": "2021-04-13T10:37:20.450780Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_audio(audio, sample_rate=8000):\n",
    "    # 加载信号\n",
    "    signal,_  = librosa.load(audio, sr=sample_rate)\n",
    "    # 产生spectrogram\n",
    "    spectro = create_spectrogram(signal)\n",
    "    # 归一化\n",
    "    means = tf.math.reduce_mean(spectro, 1, keepdims=True)\n",
    "    stddevs = tf.math.reduce_std(spectro, 1, keepdims=True)\n",
    "    X = tf.divide(tf.subtract(spectro, means), stddevs)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T10:45:51.951058Z",
     "start_time": "2021-04-13T10:45:51.825055Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = generate_audio(sample_call)\n",
    "inputs = tf.reshape(inputs, (1,397,129,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T10:47:49.031859Z",
     "start_time": "2021-04-13T10:47:33.655619Z"
    }
   },
   "outputs": [],
   "source": [
    "hidden = [tf.zeros((1, units))]\n",
    "enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "dec_hidden = enc_hidden\n",
    "\n",
    "for i in range(435):\n",
    "    dec_input = tf.random.uniform((1,1,29))\n",
    "    output = decoder(dec_input, dec_hidden, enc_out)\n",
    "    output = tf.nn.log_softmax(output)    \n",
    "    predicted_id = tf.argmax(output[0]).numpy()\n",
    "    output_text += alphabet[predicted_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T10:48:11.606463Z",
     "start_time": "2021-04-13T10:48:11.583456Z"
    }
   },
   "outputs": [],
   "source": [
    "a = []\n",
    "for l in range(len(output_text)-1):\n",
    "    if output_text[l] != output_text[l+1]:\n",
    "        a.append(output_text[l])\n",
    "a = \"\".join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T10:48:15.660978Z",
     "start_time": "2021-04-13T10:48:15.644983Z"
    }
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T10:37:26.990261Z",
     "start_time": "2021-04-13T10:37:26.981261Z"
    }
   },
   "outputs": [],
   "source": [
    "output = tf.nn.log_softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T10:37:27.328071Z",
     "start_time": "2021-04-13T10:37:27.309041Z"
    }
   },
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T10:37:30.048654Z",
     "start_time": "2021-04-13T10:37:30.044649Z"
    }
   },
   "outputs": [],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T10:37:32.319190Z",
     "start_time": "2021-04-13T10:37:32.303189Z"
    }
   },
   "outputs": [],
   "source": [
    "from string import ascii_lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T10:37:32.913694Z",
     "start_time": "2021-04-13T10:37:32.895694Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted_id = tf.argmax(output[0]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T10:37:34.882472Z",
     "start_time": "2021-04-13T10:37:34.873504Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T10:44:51.686701Z",
     "start_time": "2021-04-13T10:44:51.661702Z"
    }
   },
   "outputs": [],
   "source": [
    "# greedy decoding\n",
    "space_token = ' '\n",
    "end_token = '>'\n",
    "blank_token = '%'\n",
    "alphabet = list(ascii_lowercase) + [space_token, end_token, blank_token]\n",
    "output_text = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T10:45:06.992904Z",
     "start_time": "2021-04-13T10:45:06.978906Z"
    }
   },
   "outputs": [],
   "source": [
    "alphabet[predicted_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T10:37:43.240629Z",
     "start_time": "2021-04-13T10:37:43.104599Z"
    }
   },
   "outputs": [],
   "source": [
    "# greedy decoding\n",
    "space_token = ' '\n",
    "end_token = '>'\n",
    "blank_token = '%'\n",
    "alphabet = list(ascii_lowercase) + [space_token, end_token, blank_token]\n",
    "output_text = ''\n",
    "for timestep in output[0]:\n",
    "    output_text += alphabet[tf.math.argmax(timestep)]\n",
    "\n",
    "a = []\n",
    "for l in range(len(output_text)-1):\n",
    "    if output_text[l] != output_text[l+1]:\n",
    "        a.append(output_text[l])\n",
    "a = \"\".join(a)\n",
    "a = a.replace('%', '')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "zh-cn",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "zh-cn",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.319px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
